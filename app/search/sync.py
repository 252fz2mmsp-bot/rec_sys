"""
æ•°æ®åŒæ­¥è„šæœ¬
å°† MySQL æ•°æ®åº“ä¸­çš„å•†å“æ•°æ®åŒæ­¥åˆ° Elasticsearch
"""
from sqlalchemy.orm import Session
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from app.db.session import SessionLocal
from app.models.item import ItemInfo
from app.core.es import get_es_client
from app.core.config import settings
from app.search.index import IndexManager
import logging

logger = logging.getLogger(__name__)


class DataSyncer:
    """æ•°æ®åŒæ­¥å™¨"""
    
    def __init__(self, db: Session = None, es_client: Elasticsearch = None):
        """
        åˆå§‹åŒ–æ•°æ®åŒæ­¥å™¨
        
        Args:
            db: æ•°æ®åº“ä¼šè¯ï¼Œå¦‚æœä¸º None åˆ™åˆ›å»ºæ–°ä¼šè¯
            es_client: ES å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯
        """
        self.db = db
        self.es = es_client or get_es_client()
        self.index_name = settings.ELASTICSEARCH_INDEX_ITEMS
        self.should_close_db = db is None
    
    def _get_db(self) -> Session:
        """è·å–æ•°æ®åº“ä¼šè¯"""
        if self.db is None:
            self.db = SessionLocal()
        return self.db
    
    def _close_db(self):
        """å…³é—­æ•°æ®åº“ä¼šè¯"""
        if self.should_close_db and self.db is not None:
            self.db.close()
            self.db = None
    
    def _item_to_doc(self, item: ItemInfo) -> dict:
        """
        å°† Item æ¨¡å‹è½¬æ¢ä¸º ES æ–‡æ¡£
        
        Args:
            item: å•†å“ ORM å¯¹è±¡
            
        Returns:
            ES æ–‡æ¡£å­—å…¸
        """
        # å¤„ç† tagsï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ†å‰²ä¸ºåˆ—è¡¨
        tags = []
        if item.tags_name_list:
            if isinstance(item.tags_name_list, str):
                # å‡è®¾ tags æ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                tags = [tag.strip() for tag in item.tags_name_list.split(",") if tag.strip()]
            elif isinstance(item.tags, list):
                tags = item.tags_name_list
        
        return {
            "_index": self.index_name,
            "_id": str(item._id),  # ä½¿ç”¨ item_id ä½œä¸º ES æ–‡æ¡£ ID
            "_source": {
                "item_id": item._id,
                # "title": item.title or "",
                "group_name": item.group_name or "",
                "description": item.group_desc or "",
                "tags": tags,
                "first_level_category": item.first_level_category_name or "",
                "second_level_category": item.second_level_category_name or "",
                "created_at": item.created_at.isoformat() if item.created_at else None,
                "updated_at": item.updated_at.isoformat() if item.updated_at else None,
                # "view_count": item.view_count or 0,
                # "download_count": item.download_count or 0,
                # "like_count": item.like_count or 0
            }
        }
    
    def sync_all(self, batch_size: int = 1000, recreate_index: bool = False) -> dict:
        """
        åŒæ­¥æ‰€æœ‰å•†å“æ•°æ®åˆ° ES
        
        Args:
            batch_size: æ‰¹é‡å†™å…¥å¤§å°
            recreate_index: æ˜¯å¦é‡å»ºç´¢å¼•
            
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        try:
            db = self._get_db()
            
            # å¦‚æœéœ€è¦ï¼Œé‡å»ºç´¢å¼•
            if recreate_index:
                logger.info("Recreating index...")
                index_manager = IndexManager(self.es)
                index_manager.create_index(delete_if_exists=True)
            
            # æŸ¥è¯¢æ‰€æœ‰å•†å“ï¼ˆå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ·»åŠ åˆ†é¡µï¼‰
            logger.info("Fetching items from database...")
            items = db.query(ItemInfo).all()
            total_items = len(items)
            logger.info(f"Found {total_items} items to sync")
            
            if total_items == 0:
                return {
                    "success": True,
                    "total": 0,
                    "synced": 0,
                    "failed": 0
                }
            
            # è½¬æ¢ä¸º ES æ–‡æ¡£
            docs = [self._item_to_doc(item) for item in items]
            
            # æ‰¹é‡å†™å…¥ ES
            logger.info(f"Syncing to Elasticsearch (batch_size={batch_size})...")
            success_count, failed_items = bulk(
                self.es,
                docs,
                chunk_size=batch_size,
                raise_on_error=False,
                stats_only=False
            )
            
            # ç»Ÿè®¡ç»“æœ
            failed_count = len(failed_items) if failed_items else 0
            
            logger.info(f"âœ… Sync completed: {success_count} success, {failed_count} failed")
            
            return {
                "success": True,
                "total": total_items,
                "synced": success_count,
                "failed": failed_count,
                "failed_items": failed_items if failed_count > 0 else []
            }
            
        except Exception as e:
            logger.error(f"âŒ Sync failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
        finally:
            self._close_db()
    
    def sync_single(self, item_id: str) -> bool:
        """
        åŒæ­¥å•ä¸ªå•†å“
        
        Args:
            item_id: å•†å“ ID
            
        Returns:
            æ˜¯å¦åŒæ­¥æˆåŠŸ
        """
        try:
            db = self._get_db()
            item = db.query(ItemInfo).filter(ItemInfo._id == item_id).first()
            
            if not item:
                logger.warning(f"Item {item_id} not found in database")
                return False
            
            doc = self._item_to_doc(item)
            self.es.index(
                index=doc["_index"],
                id=doc["_id"],
                document=doc["_source"]
            )
            
            logger.info(f"âœ… Synced item {item_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to sync item {item_id}: {e}")
            return False
        finally:
            self._close_db()
    
    def delete_single(self, item_id: str) -> bool:
        """
        ä» ES åˆ é™¤å•ä¸ªå•†å“
        
        Args:
            item_id: å•†å“ ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            self.es.delete(index=self.index_name, id=str(item_id))
            logger.info(f"Deleted item {item_id} from ES")
            return True
        except Exception as e:
            logger.error(f"Failed to delete item {item_id}: {e}")
            return False


def run_sync(batch_size: int = 1000, recreate_index: bool = False):
    """
    æ‰§è¡Œæ•°æ®åŒæ­¥ï¼ˆå‘½ä»¤è¡Œå…¥å£ï¼‰
    
    Args:
        batch_size: æ‰¹é‡å†™å…¥å¤§å°
        recreate_index: æ˜¯å¦é‡å»ºç´¢å¼•
    """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    syncer = DataSyncer()
    result = syncer.sync_all(batch_size=batch_size, recreate_index=recreate_index)
    
    if result["success"]:
        print(f"\nğŸ‰ Sync completed successfully!")
        print(f"Total: {result['total']}")
        print(f"Synced: {result['synced']}")
        print(f"Failed: {result['failed']}")
    else:
        print(f"\nâŒ Sync failed: {result.get('error', 'Unknown error')}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Sync MySQL data to Elasticsearch")
    parser.add_argument("--batch-size", type=int, default=1000, help="Batch size for bulk insert")
    parser.add_argument("--recreate-index", action="store_true", help="Recreate index before syncing")
    
    args = parser.parse_args()
    run_sync(batch_size=args.batch_size, recreate_index=args.recreate_index)
